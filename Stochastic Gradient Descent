import numpy as np

# Define the function to minimize
def f(x):
    return x ** 2

# Define the derivative of the function
def df(x):
    return 2 * x

# Stochastic Gradient Descent Algorithm
def stochastic_gradient_descent(starting_point, learning_rate, num_iterations):
    x = starting_point
    for _ in range(num_iterations):
        # Simulate a random sample (in this case, it's just the same function)
        x_i = np.random.uniform(-10, 10)  # Randomly sample a point
        x -= learning_rate * df(x_i)  # Update x based on the random sample
    return x

# Parameters
starting_point = 10  # Starting point for the descent
learning_rate = 0.1   # Learning rate
num_iterations = 100   # Number of iterations

# Run Stochastic Gradient Descent
minimum = stochastic_gradient_descent(starting_point, learning_rate, num_iterations)
print(f'The minimum point is at x = {minimum} and f(x) = {f(minimum)}')
