import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize classifiers
log_reg = LogisticRegression(random_state=42)
decision_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(random_state=42)

# Create an ensemble model using Voting Classifier
voting_classifier = VotingClassifier(estimators=[
    ('log_reg', log_reg),
    ('decision_tree', decision_tree),
    ('random_forest', random_forest)],
    voting='hard'  # Use 'soft' for probability-based voting
)

# Fit the ensemble model
voting_classifier.fit(X_train, y_train)

# Evaluate the ensemble model on the test set
y_pred = voting_classifier.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)

print("Ensemble Model - Test Accuracy:", test_accuracy)

# Perform Grid Search with Cross-Validation for hyperparameter optimization
param_grid_voting = {
    'log_reg__C': [0.01, 0.1, 1, 10, 100],
    'decision_tree__max_depth': [None, 5, 10, 15],
    'random_forest__n_estimators': [10, 50, 100]
}

grid_search_voting = GridSearchCV(voting_classifier, param_grid_voting, cv=5, scoring='accuracy')
grid_search_voting.fit(X_train, y_train)

# Print the best parameters and the best score for the ensemble model
print("Voting Classifier - Best Hyperparameters:", grid_search_voting.best_params_)
print("Voting Classifier - Best Cross-Validation Score:", grid_search_voting.best_score_)
